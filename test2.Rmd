---
title: "Тестовое задание 2"
author: "Юрий Исаков"
output: html_document
---

```
Задача: исследовать данные о прохождении игроками уровней match3-игры и выполнить следующие задания:
1. Определить DAU (Daily active users).
2. Определить распределение игроков по уровням.
3. Предложить метрику, описывающую сложность уровней и рассчитать ее.
4. Если в процессе исследования будут обнаружены любые интересные факты - будет плюсом, если они будут указаны и визуализированы
```




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = TRUE)

Sys.setlocale('LC_ALL','utf-8')

library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
library(RColorBrewer)
library(dygraphs)
library(htmlwidgets)
library(htmlTable)

# цвета для графиков
green1 <- brewer.pal(6, 'Greens')[5]
green2 <- brewer.pal(6, 'Greens')[3]

# всего уровней
n_levels <- 525

```


Файл с иходными данными содержит 147,893,968 наблюдения по 4 параметра в каждом: идентификатор игрока, результат раунда, уровень раунда и дата. Всего в датасете присутствует 1,178,906 уникальных пользователей за период в 33 дня, количество уровней -- 525.


#### 1. Определить DAU (Daily active users)


Количество активных пользователей за день считалось как количество уникальных пользователей запустивших приложение и попробовавших пройти хоть один уровень хотя бы раз. На графике это более темная линия. Скорее всего, данные за 15 февраля неполные, т.к. в исходном файле даты шли не по порядку, от этого резки спад.

Пожалуй, как считать DAU это вопрос договоренности как трактовать "активность". Например, можно считать только тех пользователей, которые либо прошли хотя б один уровень, либо приложили более одной попытки (но не прошли), т.е. исключая тех, кто запустил игру всего один раз за все время и не стал пытаться ее проходить. Этот вариант соответствует светлой линии (dau_alt).

Количество активных пользователей растет со временем, также есть недельная периодичность, по воскресеньям -- характерный всплеск.

```{r dau, cache=TRUE, echo=FALSE}
dau <- fread('dau.csv')[,date :=as.Date(date)]
# head(dau)

# отображение больших чисел
valueFormatter <- "function formatValue(v) {
var suffixes = ['', 'K', 'M', 'G', 'T'];
if (v < 1000) return v;
var magnitude = Math.ceil(String(Math.floor(v)).length / 3-1);
if (magnitude > suffixes.length - 1)
magnitude = suffixes.length - 1;
return String(Math.round(v / Math.pow(10, magnitude * 3), 2)) +suffixes[magnitude]}"

dygraph(dau, 
        main = "Количество активных пользователей по дням") %>%
    dyAxis("y", label = "Количество", axisLabelFormatter = JS(valueFormatter),
           valueRange = c(0, 470000), drawGrid = TRUE) %>%
    dyAxis("x", label = "Дата", 
           valueRange = as.Date(c("2016-01-17", "2016-02-10")), 
           drawGrid = FALSE) %>%
    dyOptions(axisLineWidth = 1.5, fillGraph = TRUE,
              fillAlpha = 0.1) %>%
    dySeries("dau", color = green1, strokeWidth = 1.5, drawPoints = TRUE, 
             pointSize = 2, fillGraph = TRUE) %>%
    dySeries("dau_alt", color = green2, strokeWidth = 1.5, drawPoints = TRUE, 
             pointSize = 2, fillGraph = TRUE)

```


#### 2. Определить распределение игроков по уровням

Всего в датасете 525 уровней. В зависимости от того, как разбивать данные по столбцами (количество уровней в столбце) гистограмма может выглядеть по-разному. Здесь представлены гистограммы по 25 уровней в столбце, а уровнем игрока считается его максимальный уровень за день.

Если рассматривать распределение игроков по уровням в разрезе суток, то для каждого из дней гистограмма имеет один и тот же вид с модой на 25-50 уровнях. Например, для 19/01/2016:

```{r lvl_day, echo=FALSE, cache=TRUE, warning=FALSE}
n_levels <- 525
width <- 25
bins <- seq(0, n_levels, by=width)
lvls <- sapply(bins[1:(length(bins) - 1)], function(x) 
    paste0(x+1, '-', x+width))
lvl_day <- fread('lvl-day.csv')

# строим график
lvl_day[date == '2016-01-19'] %>% ggplot(aes(factor(lvl), n)) + 
    geom_bar(stat = 'identity', fill = green2, color = green1, width = 1) + 
    theme_minimal() + scale_x_discrete(limits=lvls) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(x = 'Уровень', y = 'Количество', 
         title = 'Типичное распределение игроков по уровням за день')
```

Если рассматривать рапределение игроков за весь доступный период, то гистограмма имеет другой вид, здесь уже наибольшая группа игроков та, которая не преодолела 25 уровень:

```{r user_levels25, cache=TRUE, echo=FALSE}
tbl_lvl25 <- fread('tbl-lvl25.csv')

tbl_lvl25 %>% ggplot(aes(factor(lvl), n)) + 
    geom_bar(stat = 'identity', fill = green2, color = green1, width = 1) + 
    theme_minimal() + scale_x_discrete(limits=tbl_lvl25$lvl) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(x = 'Уровень', y = 'Количество', 
         title = 'Распределение игроков по уровням за все время')
```

Ширина столбца съедает некоторые особенности распределения, такие как резкие перепады от уровня к уровню. Например, данные по количеству игроков с 25 по 35 уровень). 

```{r levels2030, echo=FALSE}
tbl_lvl <- fread('tbl-lvl.csv')
tbl_lvl[25:35]
```

Поэтому если отобразить те же самые данные по игрокам за все время, но с шириной столбца в один уровень, то станут заметны пики и провалы (логарифмическом мастштабе):

```{r user_levels1, cache=TRUE, echo=FALSE}
# барплот в логарифмическом масштабе
tbl_lvl %>% ggplot(aes(lvl, n+1)) + geom_bar(stat = 'identity', fill = green1, color = green1, width = 1) + scale_y_log10() + theme_minimal() + labs(y = 'Количествово', x = 'Уровень', title = 'Распределение игроков по уровням')
```

Пики и спады связаны, скорее всего, со сложностью уровней. Так, на 26 уровне застряло более 140 тысяч игроков, а на 27-28 по несколько тысяч, т.е. их оказлось легче пройти. На 29 опять резкий пик в 100 тысяч и т.д. Действительно, если посчитать корреляцию между количеством игроков, которые остались на своих уровнях и число попыток, которое было приложено для прохождения каждого уровня, то окажется, что связь очень сильная, а сам коэффициент статистически значим (здесь вычислялась корреляция Спирмена, она устойчива к выбросам и интерес представляет именно соотношение рангов этих величин).

```{r corr_spearman, echo=FALSE}
dif <- fread('dif.csv') 
cor.test(tbl_lvl$n, dif$n, method = 'spearman', exact = FALSE)
```



#### 3.	Предложить метрику, описывающую сложность уровней и рассчитать ее

При имеющихся данных логично оценить сложность уровней как отношение количества неуспешных попыток прохождения к их общему числу. Тогда график сложности от уровня будет выглядеть так:

```{r metr1, cache=TRUE, echo=FALSE}
dif <- fread('dif.csv') 
dif[c(1:5, 521:525),.(level, rate, n, f)]
ggplot(dif, aes(level, rate)) + geom_line(colour = green1) + theme_minimal()+ labs(y = 'Доля', x = 'Уровень', title = 'Доля (проигранных игр) для каждого уровня')
```

Полученное значение можно трактовать, как 1-вероятность прохождения уровня. Недостаток такой метрики видится в том, что разное количество игроков и попыток приходится на каждый уровень, поэтому границы в которых может изменяться доля -- разные. Для примера можно сравнить 17 и 524 уровень. У них близкая доля завершения уровня (0.26 и 0.22 соответственно). Наблюдений достаточно, чтобы сделать вывод о статистической значимости полученных значений, p-value очень мало (а т.к. 524 уровню в данных соответствует наименьшее количество наблюдений, то значения всех долей можно считать статистически значимыми). Ширины доверительных интервалов для обеих долей сильно отличаются:

```{r metr2, cache=TRUE, echo=FALSE}
dif[c(17, 524), .(level, f, n, rate)] 
prop.test(as.matrix(dif[level==17, .(f, n-f)]), conf.level = 0.99)
prop.test(as.matrix(dif[level==524, .(f, n-f)]), conf.level = 0.99)
```

Из-за большого количества попыток пройти 17 уровень доля его прохождения определена уверенно, но для 524 уровня это не так: сама доля равно 0.22, но ее доверительный интервал от 0.15 до 0.30 и в сравнении нет определенности какой из уровней сложнее. 

Так же на графике виден убывающий тренд, т.е. с какого-то момента (100 уровень) сложность как будто начинает уменьшаться чем дальше пользователь проходит игру.

Не зная самой игры я предположу, что все же сложность должна возрастать от уровня к уровню. Так же, видится логичной зависимость сложности от количества игроков, прошедших уровень. Для этого можно рассчитать процент прохождения уровня, т.е. отношение числа игроков, пытавшихся пройти уровень к количеству игроков, который уровень прошли (т.е. зафиксировать пользователей определенного уровня, а прошедшими считать только тех, кто из них перешел на следующий).

Таким образом, можно ввести коэффициент коррекции исходной метрики (доли) используя рассчитаные параметры. Этот коэффициент должен зависеть от следующих параметров:
* от уровня, чем дальше уровень, тем в целом игра становится сложнее (level), его большие значения зажимают начальные уровни (компенсируется шириной);
* от количества игроков, которые не смогли пройти на следующий уровень (1 - pass_rate), т.к. не все игроки в датасете находятся на одном уровне сначала;
* от ширины доверительного интервала посчитаной доли для уровня (хотя использование этого парамета спорно, он работает, если выполняется предпосылка, что сложность уровней, в целом, растет с течением игры), с его помощью можно корректировать общий тренд сложности уровней и корректировать разброс для соседних уровней.

Т.к. эти величины рассчитаны в разных масштабах, то в качестве коэффициента коррекции предлагается следующее выражение (дробь), которое чем-то похоже на взвешенное среднее гармоническое. 

$$metric = complete\_rate \cdot \frac {a \cdot level + b \cdot width + c \cdot (1-pass\_rate ) + d}{a+b+c+d}$$

Вообще говоря, ширина доверительного интервала обратно пропорциональна квадрату количества наблюдений и можно использовать его, но в ширине используется и сама доля удачных исходов и поправка Йетса, что корректирует ширину для пограничных значений. 

Если подобрать коэффициенты, то график скорректированного коэффициента сложности выглядит так (для `a=0.02, b=100, c=10, d=10`):

```{r metr3, echo=FALSE}
ggplot(dif, aes(level, rate_adjusted)) + geom_line(colour = green1) + theme_minimal()+ labs(y = 'Сложность', x = 'Уровень', title = 'Скорректированный коэффициент сложности уровня')
```

Параметры `a`, `b`, `c`, `d` скорее выбраны из эстетических соображений. Такую метрику можно откалибровать и сделать более объективной, если использовать оценки эксперта: например, рейтинги гейм-дизайнера/тестеров или рассчитать ранги сложности алгоритма (скажем, насколько часто на данном уровнем выпадают удачные комбинации уже может объяснять сложность). Имея экспертную оценку мера сложности можно уточнить используя машинное обучения или ориентируясь на ранговые методы (например, корреляцию Спирмена/Кендалла).

Возможно, имеет смысл дискретизировать полученную метрику и свести его к узкому интервалу значений, пусть от 1 до 20 (коэффициент все равно перестал быть интерпретируемым), тогда уровни удобнее сравнивать между собой, легко найти уровни одинаковой сложности, немного сложнее, существенно сложнее и т.д. В этом случае график примет вид:

```{r metr4, echo=FALSE}
ggplot(dif, aes(level, floor(rate_adjusted * 100 + 1))) + geom_line(colour = green1) + theme_minimal()+ labs(y = 'Сложность', x = 'Уровень', title = 'Мерика сложности уровня')
```

В зависимости от значения метрики сложности, уровни игры распределены следующим образом:

```{r metr5, echo=FALSE}
as.data.frame(table(dif$metr)) %>% ggplot(aes(Var1, Freq)) + geom_bar(stat = 'identity', fill = green2, color = green1, width = 1) + theme_minimal() + labs(y = 'Кол-во', x = 'Сложность', title = 'Распределение уровней по сложности')
```

Резюмируя, доля проигранных игр хорошо описывает сложность уровней, а главное -- это величина интерпретируема. Ее недостаток кроется в том, что с возрастанием уровня, его точность доли может существенно упасть. Поэтому для доли можно рассчитать коэффициент корректировки, который зависит от номера уровня, доли игроков, прошедших уровень и ширины доверительного интервала, но в этом случае желательна калибровка метрики.

#### ***

Используя данные по пользователям можно выявить тех из них, кто сильно отличается от основной массы. Здесь в небольшом примере применялся метод опорных векторов, где в качестве ядра использовалась радиальная базисная функция. В качестве исходных данных по каждому пользователю была обрабатывалас следующая информация: `level` -- уровень игрока (за все время), `progress` -- сколько уровней прошел игрок, `n` -- количество игровых раундов (попыток), `comlete_rate` -- доля успешно пройденных раундов, `days` -- количество дней в игре и `speed` -- среднее число уровней, проходимых игроком в день. Данные об одиннадцати наиболее отличающихся игроков приведены в таблице. Этих пользователей можно разделить на три группы:

* первые три игрока практически не играли, у них меньше 100 попыток, но более 200 пройденных уровне, это вполне могут быть читеры или тестеры;

* следующие 4 игрока прошли все уровни, и на каждый приходилось чуть более одной попытки, возможно это тестеры (если это настоящие игроки, игра им показалась бы слишком легкой!);

* последние 4 -- самые упроные игроки, у них более 3000 попыток прохождения, за месяц они не прошли и половины игры, а играют каждый день.

Для визуализации все данные отображены в координатах главных компонент (первые 2 компоненты описывают 80% дисперсии). "Аномальные" пользователи имеют более темный цвет.

```{r anomalocaris, echo=FALSE}
anomaly11_and_sample10000 <- fread('11-10000.csv')
anomaly11 <- fread('anomaly11.csv')[order(n)]

# вывод таблички
library(knitr)
kable(anomaly11, digits = 3)

ggplot(anomaly11_and_sample10000, aes(PC1, PC2)) + 
    geom_point(aes(colour = as.factor(anomaly))) + 
    scale_colour_manual(values = c(green2, green1)) + 
    guides(colour=FALSE) + theme_minimal() + 
    labs(y = 'ГК1', x = 'ГК2', title = '"Аномальные" игроки')
```

Перевод в главные компоненты, как видно из графика не дает внятного разделения пользователей на группы, а продвинутый метод, например, tSNE будет работать относительно долго, хотя слишком плавное изменение данных не позволяет надеяться на "красивое" разделение пользователей на кластеры. Тем не менее, как можно судить по таблице выше, каждой из групп игроков соответствуют свои особенные, отличительные значения параметров, которые можно использовать в качестве отправных точек для дальнейшей кластеризации пользователей.


```{r code, eval=FALSE, echo=FALSE}
# код, с помощью которого были обработаны данные:


########################################################################
# Чтение данных из файла
#-----------------------------------------------------------------------
nrows <- 150000000 # 
colClasses <- c('character', 'character', 'integer', 'numeric')
sample <- fread('test_fd2.csv', nrows = nrows, sep = ';', 
                colClasses = colClasses)
# event_time в дату
sample[, `:=`(date = as.Date(as.POSIXct(event_time / 1000, 
                                        origin="1970-01-01")),
              event_time = NULL)] # перевод в дату
# Read 147893968 rows and 4 (of 4) columns from 7.190 GB file in 00:01:44
#-----------------------------------------------------------------------




########################################################################
# 1. DAU
#-----------------------------------------------------------------------

# группируем по дате, пользователю, количеству игр за день
# фейлов, винов и максимальному уровню за день
user_act <- sample[
    order(date), 
    .(max_level = max(level), n = length(action),
      f = sum(action == 'f'), c = sum(action == 'c')), 
    by = .(date, uid)]
# считаем оба варианта DAU, уникальных пользоватей за день
dau <- user_act[, .(dau = length(uid),
                    dau_alt = length(uid) - sum((n == 1) & (f == 1)) ), 
                by = date]
# write.csv(dau, 'dau.csv', row.names = FALSE)
# если читать из файла, то надо преобразовать дату
# dau <- fread('dau.csv')[,date :=as.Date(date)]
#-----------------------------------------------------------------------




########################################################################
# 2. Распределение игроков по уровням
#-----------------------------------------------------------------------
# всего уровней
n_levels <- 525
# вспомогательная табличка, группировка по всем игрокам
user_levels <- sample[, .(max_lvl = max(level), min_lvl = min(level),
                          dif_lvl = max(level) - min(level)), 
                      by = uid]

##### ДЛЯ СТОЛБЦОВ В 1 УРОВЕНЬ #####
# tbl_lvl <- user_levels[,.(.N), by=max_lvl][
#     order(max_lvl), .(lvl = max_lvl, n = N)] # пропустили нули!!!
# сделаем табличку для барплота (считается дольше, но проще)
tbl_lvl <- data.table(
    lvl = 1:max(user_levels$max_lvl), 
    n = sapply(1:max(user_levels$max_lvl), 
               function(i) sum(user_levels$max_lvl == i))
    )
# write.csv(tbl_lvl, 'tbl-lvl.csv', row.names = FALSE)
# tbl_lvl <- fread('tbl-lvl.csv')

##### ДЛЯ СТОЛБЦОВ В 25 УРОВНЕЙ #####
width <- 25
bins <- seq(0, n_levels, by = width)
lvls <- sapply(bins[1:(length(bins) - 1)], function(x) 
    paste0(x+1, '-', x+width))
tbl_lvl25 <- data.table(lvl = lvls,
    n=as.numeric(table(findInterval(user_levels$max_lvl, bins, 
                                    left.open = TRUE))))
# write.csv(tbl_lvl25, 'tbl-lvl25.csv', row.names = FALSE)
# tbl_lvl25 <- fread('tbl-lvl25.csv')


##### ДЛЯ СТОЛБЦОВ В 25 УРОВНЕЙ НА КАЖДЫЙ ДЕНЬ #####
# табличка уровней пользователей по дням
# ради интереса расчет ежедневных гистограм через... жжплот ;)
lvl_day <- data.table(date = dates[0], lvl = character(0), n = numeric(0))
for (i in 1:length(dates)) {
    cat(i, ' ')
    temp <- sample[date == dates[i], .(max_lvl = max(level)), by = uid]
    # сама гистограмма
    gg <- temp %>% ggplot(aes(max_lvl)) + 
        geom_histogram(binwidth = width, center = width/2, 
                       colour="black", fill="white") +  
        theme_minimal() + xlim(0, n_levels)
    # вся информация по построенному графику теперь в ggb
    gb <- ggplot_build(gg)
    lvl_day <- rbind(lvl_day, data.table(date = dates[i], lvl=lvls, n = gb$data[[1]]$count))
}
# write.csv(lvl_day, 'lvl-day.csv', row.names = FALSE)
# lvl_day <- fread('lvl-day.csv')
#-----------------------------------------------------------------------




########################################################################
# 3. Рассчет метрики сложности
#-----------------------------------------------------------------------
# сначала считаем фейлы, попытки и количеcтво пользователей на уровне
dif <- sample[order(level), 
               .(f = sum(action == 'f'), 
                 n = length(action),
                 users = length(unique(uid))), 
               by = level]

# считаем долю
dif[,`:=`(rate = f/n, metr = round(f/n*100, 1))]

# ширины доверительных интервалов для каждого из уровней
widths <- sapply(dif$level, function(l) {
    test <- prop.test(as.matrix(dif[level==l, .(f, n-f)]), 
                      conf.level = 0.99)
    test$conf.int[2] - test$conf.int[1]
})
dif$width <- widths

# сколько из игроков, находившемся на определенном уровне перешли дальше
pass_rate <- rep(1, n_levels)
pass_rate[2:524] <- sapply(2:524, function(i) {
    lvl_cur <- sample[level == i, uid , by=uid]$uid
    lvl_next <- sample[level == i+1, uid, by=uid]$uid
    length(intersect(lvl_cur, lvl_next)) / length(lvl_cur)
})
dif$pass_rate <- pass_rate

# скорректированная доля
a <- 0.02; b <- 100; c <- 10; d <- 10
dif[, `:=`(rate_adjusted = rate * 
               (a * level + b * width + c*(1-pass_rate) + d) / 
               (a + b + c + d))]

# собственно, метрика 
dif$metr <- floor(dif$rate_adjusted * 100)+1


##### ПОДБИРАЛКА КРАСИВЫХ КОЭФФИЦИЕНТОВ #####
manipulate({
    m <- dif$rate * 
        (a * dif$level + b * dif$width + c*(1-dif$pass_rate) + d) / 
        (a + b + c + d)
    plot(dif$level, m, type = 'l')
    # barplot(m[20:40])
},
a = slider(-1, 1, 0.02, step = 0.001), b = slider(-10, 200, 100, step = 1),
c = slider(0, 100, 10, step = 0.1), d = slider(1, 50, 10, step=1))
#-----------------------------------------------------------------------




########################################################################
# ***
#-----------------------------------------------------------------------
# статистика по игрокам
user_stat <- sample[, .(level = max(level), 
                        progress = max(level) - min(level),
                        n = length(action), 
                        complete_rate = sum(action == 'c')/length(action),
                        days = length(unique(date))), 
                      by = uid]
user_stat[, `:=`(speed = progress / days)]

# выберем 10000 случайных наблюдений для графика
set.seed(19)
smpl <- sample(1:nrow(user_stat), 10000)

# преобразование в ГК для визуализации игроков
pca <- prcomp(user_stat[,cols2scale, with=F], scale. = TRUE)

# поиск аномалий с помощью опорных векторов
library(e1071)
svm_model <- svm(pca$x,  y = NULL, 
                 type = 'one-classification', 
                 nu = 1e-5, gamma = 1e-4, scale = TRUE, kernel = "radial")

# находим индексы аномалий
pred <- predict(svm_model)
idx <- which(pred == FALSE)

# данные для графика (11 аномалий и 10000 случайных игроков)
anomaly11_and_sample10000 <- as.data.frame(pca$x[c(idx, smpl), 1:2])
anomaly11_and_sample10000$anomaly <- c(rep(1, 11), rep(0, 10000))
# write.csv(anomaly11_and_sample10000, '11-10000.csv', row.names = FALSE)
# write.csv(user_stat[idx], 'anomaly11.csv', row.names = FALSE)
# anomaly11_and_sample10000 <- fread('11-10000.csv')
# anomaly11 <- fread('anomaly11.csv')[order(n)]
#-----------------------------------------------------------------------

# Fin.

```






